# 开发步骤
1. `dataset` - 数据集处理
2. `vocalubary_collate` - 构建 vocabulary collate 通过 `from torch.utils.data import DataLoader`
来载入第1步产生的 dataset，来构建 Vocabulary.
3. `model_collate` - 构建模型的实际输入，包括 padding 以及 indexing
4. `model` - 模型开发，继承自 `from easytext.model import Model`, 同时，继承
`from easytext.model import ModelOutputs` 将 loss 以及 metric 需要的数据在
`ModelOutputs` 设置。
5. `loss` - loss 开发，继承自 `from easytext.loss import Loss`, 该子类的输入是
`ModelOutputs`, 这就需要在 `model.forward` 中将在 loss 中需要信息全部填充
6. `metric` - metric开发，继承自 `from easytext.metrics import ModelMetricAdapter`
该类的输入也是 `ModelOutputs`, 在 `metric` 中需要的输入，在 `ModelOutputs` 中填充。
同时，在 `easytext.metrics` 中有很多预设好的纯的 metric, 可以在模型中使用。

# config
训练的 config 配置。`from easytext.trainer import ConfigFactory`
继承这个基类，将所有训练用的参数以字典形式返回，在训练的时候使用该工厂
创建的 config. 可以从 json 文件载入，也可以自己定义。

# 使用预训练词向量进行 fine-tuning 或者 freeze 训练

1. 继承 PretrainedVocabulary， 重写 load 方法, 来载入词向量。也可以使用默认的
载入方式，这里包括 Glovle 等词向量。
2. 在模型中使用 `torch.nn.Embedding.from_pretrained` 来载入 `PretrainedVocabulary.embedding_matrix`

下面说明在 freeze 以及 fine-tuning 两种模式下的设置:

## Freeze 模式

在 `OptimizerFactory` 需要将 embedding 参数  
`embedding.weight.requires_grad = False`, 同时在 optimizer 中，
将该参数剔除掉。其实在 optimizer 中不剔除也没关系，剔除是表示严谨。

## Fine-tuning 模式

在 `OptimizerFactory` 需要将 embedding 参数  
`embedding.weight.requires_grad = True`, 同时在 optimizer 中，
针对其学习率设置一个较小的值，因为仅仅是微调，与其他不同。